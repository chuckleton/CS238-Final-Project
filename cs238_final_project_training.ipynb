{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_path = \"C:\\\\Users\\\\dkolano\\\\OneDrive - Agile Space Industries\\\\Documents\\\\STK 12\\\\test_astrogator_collision\\\\test_astrogator_collision.sc\"\n",
    "visible = True\n",
    "userControl = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.replay_buffers import py_uniform_replay_buffer\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "from tf_agents.policies import policy_saver\n",
    "\n",
    "from tf_agents.agents.ddpg import critic_network\n",
    "from tf_agents.agents.sac import sac_agent\n",
    "from tf_agents.agents.sac import tanh_normal_projection_network\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.policies import greedy_policy\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "from tf_agents.train import actor\n",
    "from tf_agents.train import learner\n",
    "from tf_agents.train import triggers\n",
    "from tf_agents.train.utils import spec_utils\n",
    "from tf_agents.train.utils import strategy_utils\n",
    "from tf_agents.train.utils import train_utils\n",
    "\n",
    "\n",
    "import os, tempfile, shutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tempdir = tempfile.gettempdir()\n",
    "\n",
    "\n",
    "from cs238_final_project.collision_avoidance.environment import Environment\n",
    "from cs238_final_project.simulation.simulation import Simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use \"num_iterations = 1e6\" for better results (2 hrs)\n",
    "# 1e5 is just so this doesn't take too long (1 hr)\n",
    "num_iterations = 55000  # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 10  # @param {type:\"integer\"}\n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_capacity = 1000000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "\n",
    "critic_learning_rate = 4e-4  # @param {type:\"number\"}\n",
    "actor_learning_rate = 4e-4  # @param {type:\"number\"}\n",
    "alpha_learning_rate = 4e-4  # @param {type:\"number\"}\n",
    "target_update_tau = 0.005  # @param {type:\"number\"}\n",
    "target_update_period = 1  # @param {type:\"number\"}\n",
    "gamma = 1.0  # @param {type:\"number\"}\n",
    "reward_scale_factor = 1.0  # @param {type:\"number\"}\n",
    "\n",
    "actor_fc_layer_params = (32, 16)\n",
    "critic_joint_fc_layer_params = (32, 16)\n",
    "\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 8  # @param {type:\"integer\"}\n",
    "eval_interval = 2000  # @param {type:\"integer\"}\n",
    "\n",
    "policy_save_interval = eval_interval  # @param {type:\"integer\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = Simulation.simulation_from_file(\n",
    "    scenario_path, use_stk_engine=False,\n",
    "    visible=visible, userControl=userControl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = Environment(sim, continuous=True)\n",
    "collect_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "# collect_env = train_py_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_spec = collect_env.observation_spec()\n",
    "action_spec = collect_env.action_spec()\n",
    "\n",
    "critic_net = critic_network.CriticNetwork(\n",
    "    (observation_spec, action_spec),\n",
    "    observation_fc_layer_params=None,\n",
    "    action_fc_layer_params=None,\n",
    "    joint_fc_layer_params=critic_joint_fc_layer_params,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    last_kernel_initializer='glorot_uniform')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "    observation_spec,\n",
    "    action_spec,\n",
    "    fc_layer_params=actor_fc_layer_params,\n",
    "    continuous_projection_net=(\n",
    "        tanh_normal_projection_network.TanhNormalProjectionNetwork))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "tf_agent = sac_agent.SacAgent(\n",
    "    collect_env.time_step_spec(),\n",
    "    action_spec,\n",
    "    actor_network=actor_net,\n",
    "    critic_network=critic_net,\n",
    "    actor_optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=actor_learning_rate),\n",
    "    critic_optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=critic_learning_rate),\n",
    "    alpha_optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=alpha_learning_rate),\n",
    "    target_update_tau=target_update_tau,\n",
    "    target_update_period=target_update_period,\n",
    "    td_errors_loss_fn=tf.math.squared_difference,\n",
    "    gamma=gamma,\n",
    "    reward_scale_factor=reward_scale_factor,\n",
    "    train_step_counter=train_step)\n",
    "\n",
    "tf_agent.initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = greedy_policy.GreedyPolicy(tf_agent.policy)\n",
    "collect_policy = tf_agent.collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spec = tf_agent.collect_data_spec\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec, batch_size=collect_env.batch_size, max_length=replay_buffer_capacity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = tf_metrics.NumberOfEpisodes()\n",
    "env_steps = tf_metrics.EnvironmentSteps()\n",
    "ave_return = tf_metrics.AverageReturnMetric()\n",
    "\n",
    "num_episodes_eval = tf_metrics.NumberOfEpisodes()\n",
    "env_steps_eval = tf_metrics.EnvironmentSteps()\n",
    "ave_return_eval = tf_metrics.AverageReturnMetric()\n",
    "\n",
    "observers = [replay_buffer.add_batch, num_episodes,\n",
    "             env_steps, ave_return]\n",
    "observers_eval = [num_episodes_eval, env_steps_eval,\n",
    "                  ave_return_eval]\n",
    "\n",
    "def reset_eval_metrics():\n",
    "    for o in observers_eval:\n",
    "        o.reset()\n",
    "\n",
    "\n",
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, sample_batch_size=batch_size, num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    collect_env,\n",
    "    collect_policy,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    num_steps=initial_collect_steps)\n",
    "initial_collect_driver.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    collect_env,\n",
    "    collect_policy,\n",
    "    observers=observers,\n",
    "    num_steps=collect_steps_per_iteration)\n",
    "\n",
    "eval_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    collect_env,\n",
    "    eval_policy,\n",
    "    observers=observers_eval,\n",
    "    num_episodes=num_eval_episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(os.path.join(\n",
    "    tempdir, learner.POLICY_SAVED_MODEL_DIR), ignore_errors=True)\n",
    "shutil.rmtree(os.path.join(tempdir, 'checkpoint'), ignore_errors=True)\n",
    "shutil.rmtree(os.path.join(tempdir, 'checkpoints'), ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_dir = os.path.join(tempdir, learner.POLICY_SAVED_MODEL_DIR)\n",
    "\n",
    "# Triggers to save the agent's policy checkpoints.\n",
    "learning_triggers = [\n",
    "    triggers.PolicySavedModelTrigger(\n",
    "        saved_model_dir,\n",
    "        tf_agent,\n",
    "        train_step,\n",
    "        interval=policy_save_interval),\n",
    "    triggers.StepPerSecondLogTrigger(train_step, interval=1000),\n",
    "]\n",
    "\n",
    "\n",
    "def experience_dataset_fn(): return dataset\n",
    "\n",
    "\n",
    "agent_learner = learner.Learner(\n",
    "    tempdir,\n",
    "    train_step,\n",
    "    tf_agent,\n",
    "    experience_dataset_fn,\n",
    "    triggers=learning_triggers)\n",
    "\n",
    "\n",
    "checkpoint_dir = os.path.join(tempdir, 'checkpoint')\n",
    "train_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=checkpoint_dir,\n",
    "    max_to_keep=1,\n",
    "    agent=tf_agent,\n",
    "    policy=tf_agent.policy,\n",
    "    replay_buffer=replay_buffer,\n",
    "    global_step=train_step\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_metrics():\n",
    "  eval_driver.run()\n",
    "  results = {}\n",
    "  for metric in observers_eval:\n",
    "    results[metric.name] = metric.result()\n",
    "  return results\n",
    "\n",
    "\n",
    "# metrics = get_eval_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_eval_metrics(step, metrics):\n",
    "  eval_results = (', ').join(\n",
    "      '{} = {:.6f}'.format(name, result) for name, result in metrics.items())\n",
    "  print('step = {0}: {1}'.format(step, eval_results))\n",
    "\n",
    "\n",
    "# log_eval_metrics(0, metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(losses, returns, training_returns):\n",
    "  plt.figure(figsize=(16, 6))\n",
    "  plt.subplot(1, 3, 1)\n",
    "  plt.plot(losses)\n",
    "  plt.title('Training Loss')\n",
    "  plt.subplot(1, 3, 2)\n",
    "  plt.plot(returns)\n",
    "  plt.title('Evaluation Average Returns')\n",
    "  ax = plt.gca()\n",
    "  ax.set_ylim([ax.get_ylim()[0], 0])\n",
    "  plt.subplot(1, 3, 3)\n",
    "  plt.plot(training_returns)\n",
    "  plt.title('Average Training Returns')\n",
    "  ax = plt.gca()\n",
    "  ax.set_ylim([ax.get_ylim()[0], 0])\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reset the train step\n",
    "tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = get_eval_metrics()[\"AverageReturn\"]\n",
    "returns = [avg_return]\n",
    "losses = []\n",
    "training_returns = []\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "  # Training.\n",
    "  collect_driver.run()\n",
    "  loss_info = agent_learner.run(iterations=1)\n",
    "  # Evaluating.\n",
    "  step = agent_learner.train_step_numpy\n",
    "\n",
    "  if eval_interval and step % eval_interval == 0:\n",
    "    train_checkpointer.save(train_step)\n",
    "    reset_eval_metrics()\n",
    "    metrics = get_eval_metrics()\n",
    "    log_eval_metrics(step, metrics)\n",
    "    returns.append(metrics[\"AverageReturn\"])\n",
    "    plot_metrics(losses, returns, training_returns)\n",
    "\n",
    "  if log_interval and step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}, average return: {2}'.format(\n",
    "        step, loss_info.loss.numpy(), ave_return.result().numpy()))\n",
    "    training_returns.append(ave_return.result().numpy())\n",
    "    losses.append(loss_info.loss.numpy())\n",
    "\n",
    "print('Number of Steps: ', env_steps.result().numpy())\n",
    "print('Number of Episodes: ', num_episodes.result().numpy())\n",
    "print('Average Return: ', ave_return.result().numpy())\n",
    "\n",
    "plot_metrics(losses, returns, training_returns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "plt.style.use('seaborn-dark')\n",
    "\n",
    "\n",
    "def plot_state(x, y, new_figure=True):\n",
    "    if new_figure:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.plot([0], [0], 'o', label='Target', markersize=12, color='black')\n",
    "    plt.plot(x, y, '--', label='Agent Path', linewidth=3, color='blue')\n",
    "    plt.plot(x[-1], y[-1], 'o', label='Initial State',\n",
    "             markersize=12, color='red')\n",
    "    plt.plot(x[0], y[0], '*', label='Final State',\n",
    "             markersize=15, color='green')\n",
    "    if new_figure:\n",
    "        plt.legend()\n",
    "        plt.title('Agent Paths on Three Representative Tests', fontsize=20)\n",
    "        plt.xlabel('$250(\\\\lambda_t - \\\\lambda_a)$', fontsize=16)\n",
    "        plt.ylabel('$a_t-a_a$', fontsize=16)\n",
    "        plt.xticks(fontsize=15)\n",
    "        plt.yticks(fontsize=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(losses, returns):\n",
    "  plt.style.use('ggplot')\n",
    "  plt.figure(figsize=(14, 7))\n",
    "  plt.subplot(1, 2, 1)\n",
    "  x = np.arange(len(losses))*200\n",
    "  plt.plot(x, losses, linewidth=2)\n",
    "  plt.title('Training Loss', fontsize=20)\n",
    "  plt.ylabel('Training Loss', fontsize=14)\n",
    "  plt.xlabel('Training Step', fontsize=14)\n",
    "  plt.xticks(fontsize=12)\n",
    "  plt.yticks(fontsize=12)\n",
    "  plt.subplot(1, 2, 2)\n",
    "  x = np.arange(len(returns))*2000\n",
    "  plt.plot(x, returns, linewidth=2)\n",
    "  plt.title('Evaluation Average Returns', fontsize=20)\n",
    "  plt.xlabel('Training Step', fontsize=14)\n",
    "  plt.ylabel('Average Evaluation Return', fontsize=14)\n",
    "  plt.xticks(fontsize=12)\n",
    "  plt.yticks(fontsize=12)\n",
    "  plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c1de19f4b928a5953380ec48474d913af54134a2d89381f588aa0f8c386c90cc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
